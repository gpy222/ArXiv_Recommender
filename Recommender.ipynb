{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This module requires that the ArXiv dataset has been downloaded and converted to a \n",
    "# Pandas DataFrame in order to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(article_data):\n",
    "    \n",
    "    article_data.drop_duplicates(subset = ['id'], keep = 'first', inplace = True)\n",
    "    \n",
    "    article_data['categories_new'] = list(article_data.categories.str.split(' '))\n",
    "    \n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_data(article_data):\n",
    "    \n",
    "    #remove all non-alpha characters\n",
    "    authors_stripped = [[re.sub('[^a-z]', '', (author[0] + author[1]).lower()) for author in ls] for ls in article_data.authors_parsed]\n",
    "    \n",
    "    #remove empty strings\n",
    "    authors_stripped = [[author for author in ls if author] for ls in authors_stripped]\n",
    "    \n",
    "    assert(len(authors_stripped) == len(article_data))\n",
    "    \n",
    "    article_data['authors_stripped'] = authors_stripped\n",
    "    article_data['num_authors'] = [len(ls) for ls in article_data.authors_stripped]\n",
    "    \n",
    "    authors_array = np.array([article_data.id, authors_stripped]).T\n",
    "    authors_dict = {}\n",
    "    for article in authors_array:\n",
    "        ID = article[0]\n",
    "        for author in article[1]:\n",
    "            if author in authors_dict.keys():\n",
    "                authors_dict[author].append(ID)\n",
    "            \n",
    "            else:\n",
    "                authors_dict[author] = [ID]\n",
    "                \n",
    "    authors_data = pd.DataFrame()\n",
    "    authors_data['Authors'] = authors_dict.keys()\n",
    "    authors_data['Articles'] = authors_dict.values()\n",
    "    authors_data['Num_Articles'] = [len(articles) for articles in authors_data.Articles]\n",
    "    \n",
    "    return authors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(article_data):\n",
    "    \n",
    "    categories_set = list(set([category for categories in article_data.categories_new for category in categories]))\n",
    "    \n",
    "    article_dictionary = dict()\n",
    "\n",
    "    for category in categories_set:\n",
    "    \n",
    "        of_interest = [category in categories for categories in article_data.categories_new]\n",
    "    \n",
    "        article_dictionary[category] = article_data[of_interest]\n",
    "    \n",
    "        article_dictionary[category].reset_index(inplace = True)\n",
    "        \n",
    "    return article_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(article_data):\n",
    "    \n",
    "    categories_set = list(set([category for categories in article_data.categories_new for category in categories]))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', max_df = .9)\n",
    "    \n",
    "    text_vectorized = dict()\n",
    "    \n",
    "    for category in categories_set:\n",
    "        of_interest = [category in categories for categories in article_data.categories_new]\n",
    "        articles_of_interest = article_data[of_interest]\n",
    "    \n",
    "        corpus = list(articles_of_interest.abstract)\n",
    "    \n",
    "        text_vectorized[category] = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "    return text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(to_ids, text_vectorized, article_data):\n",
    "    \n",
    "    N = len(article_data)\n",
    "    M = len(to_ids)\n",
    "    similarity_array = np.empty((N,M))\n",
    "\n",
    "    for i in range(M):\n",
    "        \n",
    "        index = list(article_data[article_data.id == to_ids[i]].index)[0]\n",
    "    \n",
    "        similarity = np.dot(text_vectorized, text_vectorized[index].T)\n",
    "        similarity = similarity.todense()\n",
    "        similarity += 1\n",
    "        similarity_array[:, i] = np.squeeze(similarity)\n",
    "    \n",
    "    similarity_score = M / np.sum(1/similarity_array, axis = 1)\n",
    "    similarity_score += -1\n",
    "    \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(ids, vector_dictionary, article_dictionary):\n",
    "    \n",
    "    categories_list = list()\n",
    "    for article_id in ids:\n",
    "        categories_list += list(article_data[article_data.id == article_id].categories_new)[0]\n",
    "        \n",
    "    categories_list = list(set(categories_list))\n",
    "    \n",
    "    recommended = dict()\n",
    "    \n",
    "    for category in categories_list:\n",
    "        \n",
    "        articles = article_dictionary[category]\n",
    "        vectors = vector_dictionary[category]\n",
    "        \n",
    "        indices = articles[articles['id'].isin(ids)].index\n",
    "    \n",
    "        df = pd.DataFrame()\n",
    "        df['id'] = articles.id\n",
    "        df['similarity_score'] = cosine_similarity(ids, vectors, articles)\n",
    "        \n",
    "        df.sort_values(by = 'similarity_score', ascending = False, inplace = True)\n",
    "        df.drop(indices, inplace = True)\n",
    "        recommended[category] = df.head(10)\n",
    "    \n",
    "    recommendations = pd.concat(recommended.values())\n",
    "    recommendations.sort_values(by = 'similarity_score', ascending = False, inplace = True)\n",
    "    recommendations.drop_duplicates(subset = ['id'], keep = 'first', inplace = True)\n",
    "    \n",
    "    return recommendations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self, article_data, fresh = True):\n",
    "        \n",
    "        if fresh:\n",
    "            self.articles = preprocess(article_data)\n",
    "            self.articles_categorized = categorize(self.articles)\n",
    "            self.authors = create_author_data(self.articles)\n",
    "            self.vectors = vectorize(self.articles)\n",
    "        \n",
    "            with open('ArXiv/articles_categorized.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.articles_categorized, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            self.authors.to_pickle('ArXiv/authors.pkl')\n",
    "            \n",
    "            with open('ArXiv/vectors.pkl', 'wb') as handle:\n",
    "                pickle.dump(self.vectors, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        else: \n",
    "            with open('ArXiv/articles_categorized.pkl', 'rb') as handle:\n",
    "                data = pickle.load(handle)\n",
    "            self.articles_categorized = data\n",
    "            \n",
    "            self.authors = pd.read_pickle('ArXiv/authors.pkl')\n",
    "            \n",
    "            with open('ArXiv/vectors.pkl', 'rb') as handle:\n",
    "                data = pickle.load(handle)\n",
    "            self.vectors = data\n",
    "            \n",
    "    def recommend(self, ids):\n",
    "        \n",
    "        results = text_similarity(ids, self.vectors, self.articles_categorized)\n",
    "        \n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
